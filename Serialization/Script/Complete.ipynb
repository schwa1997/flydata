{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in c:\\python312\\lib\\site-packages (7.1.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\python312\\lib\\site-packages (from rdflib) (3.2.0)\n",
      "Requirement already satisfied: meteostat in c:\\python312\\lib\\site-packages (1.6.8)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\python312\\lib\\site-packages (from meteostat) (2.2.3)\n",
      "Requirement already satisfied: pytz in c:\\python312\\lib\\site-packages (from meteostat) (2024.2)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from meteostat) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas>=1.1->meteostat) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas>=1.1->meteostat) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1->meteostat) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install rdflib\n",
    "%pip install meteostat\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD, RDFS\n",
    "from datetime import datetime, timedelta\n",
    "from meteostat import Point, Hourly\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path(r\"D:\\\\OneDrive - Universit√† degli Studi di Padova\\\\Lezioni\\\\Magistrale\\\\Terzo semestre\\\\Graph DB\\\\Homework\\\\flydata\")\n",
    "AIRPORTS_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"airports.csv\"\n",
    "CITIES_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"cities.csv\"\n",
    "FLIGHTS_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"flights.csv\"\n",
    "OUTPUT_PATH_AIRPORT = BASE_PATH / \"Serialization\" /\"ttl\"/ \"airport.ttl\"\n",
    "OUTPUT_PATH_CITY = BASE_PATH / \"Serialization\" /\"ttl\"/ \"city.ttl\"\n",
    "OUTPUT_PATH_WEATHER = BASE_PATH / \"Serialization\" /\"ttl\"/ \"weather.ttl\"\n",
    "OUTPUT_PATH_FLIGHTS = BASE_PATH / \"Serialization\" /\"ttl\"/ \"flights.ttl\"\n",
    "OUTPUT_PATH_ROUTES = BASE_PATH / \"Serialization\" /\"ttl\"/ \"routes.ttl\"\n",
    "\n",
    "\n",
    "# Define namespaces\n",
    "FDO = Namespace(\"http://www.semanticweb.org/nele/ontologies/2024/10/flydata/\")\n",
    "\n",
    "# Define the time interval for the weather data\n",
    "start = pd.to_datetime('2024-8-1')\n",
    "end = pd.to_datetime('2024-8-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nc4786862cd424edba65fe071f71449b6 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_city = Graph()\n",
    "\n",
    "g_city.bind(\"fdo\", FDO)\n",
    "g_city.bind(\"xsd\", XSD)\n",
    "\n",
    "cities = {}\n",
    "try:\n",
    "    # Load the CSV file in memory using pandas\n",
    "    cities_df = pd.read_csv(CITIES_PATH)\n",
    "    for _, row in cities_df.iterrows():\n",
    "        # Convert city_ascii to string and skip if it's NaN\n",
    "        if pd.isna(row['city_ascii']):\n",
    "            continue\n",
    "            \n",
    "        city_ascii = str(row['city_ascii'])\n",
    "        cities[city_ascii] = {\n",
    "            'name': row['city'],\n",
    "            'population': str(row['population']) if pd.notna(row['population']) else \"0\"\n",
    "        }\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Add cities to the city graph\n",
    "for city_ascii, data in cities.items():\n",
    "    # Remove special characters and spaces, then URL encode\n",
    "    city_id = quote(city_ascii.encode('ascii', 'ignore').decode().replace(\" \", \"_\").replace(\"'\", \"\").replace(\",\", \"\"))\n",
    "    city_uri = URIRef(str(FDO) + city_id)\n",
    "    \n",
    "    # Add city triples\n",
    "    g_city.add((city_uri, RDF.type, FDO.City))\n",
    "    g_city.add((city_uri, FDO.name, Literal(data['name'], datatype=XSD.string)))\n",
    "    g_city.add((city_uri, FDO.population, Literal(data['population'], datatype=XSD.string)))\n",
    "\n",
    "g_city.serialize(destination=str(OUTPUT_PATH_CITY), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N9c1148d918d1402ea24f170e1e7b3e3a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_airport = Graph()\n",
    "\n",
    "g_airport.bind(\"fdo\", FDO)\n",
    "g_airport.bind(\"xsd\", XSD)\n",
    "\n",
    "# Define the isLocatedInCity property in the airport graph\n",
    "g_airport.add((FDO.isLocatedInCity, RDF.type, RDF.Property))\n",
    "g_airport.add((FDO.isLocatedInCity, RDFS.domain, FDO.Airport))\n",
    "g_airport.add((FDO.isLocatedInCity, RDFS.range, FDO.City))\n",
    "\n",
    "airports = []\n",
    "try:\n",
    "    with open(AIRPORTS_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['IATA']:  # Only include airports with IATA code\n",
    "                airports.append({\n",
    "                    'name': row['Name'].strip('\"'),\n",
    "                    'city': row['City'].strip('\"'),\n",
    "                    'iata': row['IATA'].strip('\"'),\n",
    "                    'lat': row['LAT'].strip('\"'),\n",
    "                    'long': row['LONG'].strip('\"')\n",
    "                })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading airports file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Add airports and their relationships to the airport graph\n",
    "for airport in airports:\n",
    "    # Use IATA code directly in the URI\n",
    "    airport_uri = URIRef(str(FDO) + airport['iata'])\n",
    "    \n",
    "    # Add airport triples\n",
    "    g_airport.add((airport_uri, RDF.type, FDO.Airport))\n",
    "    g_airport.add((airport_uri, FDO.name, Literal(airport['name'], datatype=XSD.string)))\n",
    "    \n",
    "    # Only add isLocatedInCity if the city exists in our cities dataset\n",
    "    if airport['city'] in cities:\n",
    "        city_id = quote(airport['city'].encode('ascii', 'ignore').decode().replace(\" \", \"_\").replace(\"'\", \"\").replace(\",\", \"\"))\n",
    "        city_uri = URIRef(str(FDO) + city_id)\n",
    "        g_airport.add((airport_uri, FDO.isLocatedInCity, city_uri))\n",
    "\n",
    "# Serialize both graphs to separate TTL files\n",
    "g_airport.serialize(destination=str(OUTPUT_PATH_AIRPORT), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression of the meteostat warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "g_weather = Graph()\n",
    "\n",
    "g_weather.bind(\"fdo\", FDO)\n",
    "g_weather.bind(\"xsd\", XSD)\n",
    "\n",
    "# for each airport, get the weather data\n",
    "index = 0\n",
    "for airport in airports:\n",
    "    # Try to get the weather data for each airport\n",
    "    try:   \n",
    "        # Get the weather data\n",
    "        point = Point(float(airport['lat']), float(airport['long']), 0)\n",
    "        data = Hourly(point, start, end)\n",
    "        data = data.fetch()\n",
    "        \n",
    "        # Create the node to add to the Graph\n",
    "        for idx, row in data.iterrows():\n",
    "            # the node has the namespace + the airport iata + timestamp as URI\n",
    "            airport_uri = URIRef(str(FDO) + airport['iata'] + str(int(row.name.timestamp()/1000)))\n",
    "            \n",
    "            # Add airport triples\n",
    "            g_weather.add((airport_uri, RDF.type, FDO.Weather))\n",
    "            g_weather.add((airport_uri, FDO['weatherDate'], Literal(int(row.name.timestamp()), datatype=XSD.dateTime)))\n",
    "            coco_standardized = 0 if pd.isna(row.coco) else int(row.coco)\n",
    "            g_weather.add((airport_uri, FDO['weatherType'], Literal(coco_standardized, datatype=XSD.nonNegativeInteger)))\n",
    "            g_weather.add((airport_uri, FDO['hasAirport'], URIRef(FDO[airport['iata']])))\n",
    "    except Exception as e:\n",
    "        print(f\"Error weather ({airport['iata']}): {str(e)}\")\n",
    "\n",
    "    print(f\"{int(index/len(airports)*100)} % complete\\r\", end=\"\\r\")\n",
    "    index+=1\n",
    "\n",
    "print(f\"serialization\\r\", end=\"finished\")\n",
    "g_weather.serialize(destination=str(OUTPUT_PATH_WEATHER), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights and routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_flights = Graph()\n",
    "g_routes = Graph()\n",
    "\n",
    "g_flights.bind(\"fdo\", FDO)\n",
    "g_flights.bind(\"xsd\", XSD)\n",
    "g_routes.bind(\"fdo\", FDO)\n",
    "g_routes.bind(\"xsd\", XSD)\n",
    "\n",
    "timefieldsontology = {\n",
    "        'Scheduled departure time as shown in Official Airline Guide(OAG)': 'ScheduledDepartureOAGTime',\n",
    "        'Scheduled departure time as shown in CRS(selected by the Carrier)': 'ScheduledDepartureCRSTime',\n",
    "        'Gate departure time (actual)': 'ActualGateDepartureTime',\n",
    "        'Scheduled arrival time per OAG': 'ScheduledArrivalTimePerOAG',\n",
    "        'Scheduled arrival time per CRS': 'ScheduledArrivalTimePerCRS',\n",
    "        'Gate arrival time (actual)': 'ActualGateArrivalTime',\n",
    "        'Wheels-off time (actual)': 'ActualWheels-offTime',\n",
    "        'Wheels-on time (actual)': 'ActualWheels-onTime'\n",
    "    }\n",
    "minutesfieldsontology = {\n",
    "    'Scheduled elapsed time per CRS': 'ScheduledElapsedTimePerCRS',\n",
    "    'Actual gate-to-gate time': 'ActualGate-to-gateTime',\n",
    "    'Departure delay time (actual minutes)': 'ActualDepartureDelayTime',\n",
    "    'Arrival delay time (actual minutes)': 'ActualArrivalDelayTime',\n",
    "    'Elapsed time difference (actual minutes)': 'ActualElapsedTimeDifference',\n",
    "    'Minutes late for Delay Code E - Carrier Caused': 'LateE',\n",
    "    'Minutes late for Delay Code F - Weather': 'LateF',\n",
    "    'Minutes late for Delay Code G - National Aviation System (NAS)': 'LateG',\n",
    "    'Minutes late for Delay Code H - Security': 'LateH',\n",
    "    'Minutes late for Delay Code I - Late Arriving Flight (Initial)': 'LateI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Load the CSV file in memory using pandas\n",
    "    flights_df = pd.read_csv(FLIGHTS_PATH, dtype={\"Scheduled Operating Carrier Code\": \"string\", \"Date of flight operation\": \"string\"})\n",
    "    # Fill NaN values in \"Actual Operating Carrier Flight Number\" with 0 and convert to int\n",
    "    flights_df[\"Actual Operating Carrier Flight Number\"] = flights_df[\"Actual Operating Carrier Flight Number\"].fillna(0).astype(int)\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "index = 0\n",
    "# Add flights to the flights graph\n",
    "for idx, row in flights_df.iterrows():\n",
    "\n",
    "    route_id = quote(str(row['Departure airport code']) + str(row['Arrival airport code']))\n",
    "    route_uri = URIRef(str(FDO) + route_id)\n",
    "\n",
    "    # Add route triples\n",
    "    # check if the route already exists\n",
    "    if (route_uri, RDF.type, FDO.Route) not in g_routes:\n",
    "        g_routes.add((route_uri, RDF.type, FDO.Route))\n",
    "        g_routes.add((route_uri, FDO['hasDepartureAirport'], URIRef(FDO[row['Departure airport code']])))\n",
    "        g_routes.add((route_uri, FDO['hasArrivalAirport'], URIRef(FDO[row['Arrival airport code']])))\n",
    "    \n",
    "    flight_date = datetime.strptime(row['Date of flight operation'], \"%m/%d/%Y\")\n",
    "    flight_id = quote(str(row['Actual Operating Carrier Code']) + str(row['Actual Operating Carrier Flight Number']) + flight_date.strftime(\"%Y%m%d\"))\n",
    "    flight_uri = URIRef(str(FDO) + flight_id)\n",
    "    \n",
    "    start_time = row['Scheduled departure time as shown in Official Airline Guide(OAG)']\n",
    "    for time in timefieldsontology.keys():\n",
    "        if pd.isna(row[time]) or row[time] == 0:\n",
    "            row[time] = None\n",
    "            continue\n",
    "        # convert the time to a datetime object\n",
    "        m = int(str(int(row[time]))[-2:])\n",
    "        h = int(str(int(row[time]))[:2]) if len(str(row[time])) == 4 else 0\n",
    "        # check if the time is in the next day\n",
    "        d = 1 if row[time] < start_time-200 else 0\n",
    "        row[time] = flight_date + timedelta(days=d, hours=h, minutes=m)\n",
    "    \n",
    "    # Add flights triples\n",
    "    g_flights.add((flight_uri, RDF.type, FDO.Flight))\n",
    "    g_flights.add((flight_uri, FDO['hasAircraft'], URIRef(FDO[str(row['Aircraft tail number'])])))\n",
    "    g_flights.add((flight_uri, FDO['hasRoute'], route_uri))\n",
    "    g_flights.add((flight_uri, FDO['isOperatedBy'], URIRef(FDO[str(row['Actual Operating Carrier Code'])])))\n",
    "    if not pd.isna(row['Cancellation code']):\n",
    "        g_flights.add((flight_uri, FDO['CancellationCode'], Literal(row['Cancellation code'], datatype=XSD.string)))\n",
    "    g_flights.add((flight_uri, FDO['flightDate'], Literal(flight_date, datatype=XSD.dateTime)))\n",
    "    \n",
    "    \n",
    "    # time fields\n",
    "    for original, ontology in timefieldsontology.items():\n",
    "        if not pd.isna(row[original]):\n",
    "            g_flights.add((flight_uri, FDO[ontology], Literal(row[original], datatype=XSD.dateTime)))\n",
    "    # minutes fields\n",
    "    for original, ontology in minutesfieldsontology.items():\n",
    "        if not pd.isna(row[original]):\n",
    "            g_flights.add((flight_uri, FDO[ontology], Literal(int(row[original]), datatype=XSD.integer)))\n",
    "    \n",
    "    index+=1\n",
    "    print(f\"{int(index/len(flights_df)*100)} % complete\", end=\"\\r\")\n",
    "\n",
    "print(f\"serialization\\r\")\n",
    "g_flights.serialize(destination=str(OUTPUT_PATH_FLIGHTS), format='turtle')\n",
    "g_routes.serialize(destination=str(OUTPUT_PATH_ROUTES), format='turtle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

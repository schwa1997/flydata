{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib\n",
    "%pip install meteostat\n",
    "%pip install geopy\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD, RDFS, WGS\n",
    "from datetime import datetime, timedelta\n",
    "from meteostat import Point, Hourly\n",
    "import warnings\n",
    "import hashlib \n",
    "from geopy.distance import geodesic as GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# Input paths\n",
    "BASE_PATH = Path(r\"D:\\\\OneDrive - Universit√† degli Studi di Padova\\\\Lezioni\\\\Magistrale\\\\Terzo semestre\\\\Graph DB\\\\Homework\\\\flydata\")\n",
    "STATE_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"states.csv\"\n",
    "CITY_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"cities.csv\"\n",
    "AIRPORT_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"airports.csv\"\n",
    "CARRIER_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"carriers.csv\"\n",
    "AIRCRAFT_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"aircrafts.csv\"\n",
    "MODEL_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"model.csv\"\n",
    "FLIGHT_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"flight.csv\"\n",
    "MANUFACTURER_PATH = BASE_PATH / \"DataCollection\" / \"CSVData\" / \"manufacturer.csv\"\n",
    "# Output paths\n",
    "OUTPUT_AIRPORT_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"airports.ttl\"\n",
    "OUTPUT_CARRIER_PATH = BASE_PATH / \"Serialization\" / \"ttl\" / \"carriers.ttl\"\n",
    "OUTPUT_STATE_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"states.ttl\"\n",
    "OUTPUT_CITY_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"cities.ttl\"\n",
    "OUTPUT_AIRCRAFT_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"aircrafts.ttl\"\n",
    "OUTPUT_MODEL_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"models.ttl\"\n",
    "OUTPUT_WEATHER_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"weather.ttl\"\n",
    "OUTPUT_FLIGHT_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"flights.ttl\"\n",
    "OUTPUT_ROUTE_PATH = BASE_PATH / \"Serialization\" /\"ttl\"/ \"routes.ttl\"\n",
    "OUTPUT_MANUFACTURER_PATH = BASE_PATH / \"Serialization\" / \"ttl\" / \"manufacturers.ttl\"\n",
    "\n",
    "# Define namespaces\n",
    "FDO = Namespace(\"http://www.semanticweb.org/nele/ontologies/2024/10/flydata/\")\n",
    "\n",
    "# Define the time interval for the weather data\n",
    "start = pd.to_datetime('2024-8-1')\n",
    "end = pd.to_datetime('2024-8-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_state = Graph()\n",
    "\n",
    "g_state.bind(\"fdo\", FDO)\n",
    "g_state.bind(\"xsd\", XSD)\n",
    "\n",
    "states = []\n",
    "try:\n",
    "    with open(STATE_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['State'] and row['Abbreviation']:  # Need both fields\n",
    "                state_dict = {\n",
    "                    'name': row['State'].strip(),\n",
    "                    'abbreviation': row['Abbreviation'].strip()\n",
    "                }\n",
    "                states.append(state_dict)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading states file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Create a mapping of state abbreviations to URIs\n",
    "for state in states:\n",
    "    state_id = quote(state['name'].encode('ascii', 'ignore').decode().replace(\" \", \"_\").replace(\"'\", \"\").replace(\",\", \"\"))\n",
    "    g_state.add((URIRef(str(FDO) + state_id), RDF.type, FDO.State))\n",
    "    g_state.add((URIRef(str(FDO) + state_id), FDO.name, Literal(state['name'], datatype=XSD.string)))\n",
    "    g_state.add((URIRef(str(FDO) + state_id), FDO.abbreviation, Literal(state['abbreviation'], datatype=XSD.string)))\n",
    "\n",
    "g_state.serialize(destination=str(OUTPUT_STATE_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_city = Graph()\n",
    "\n",
    "g_city.bind(\"fdo\", FDO)\n",
    "g_city.bind(\"xsd\", XSD)\n",
    "g_city.bind(\"wgs\", WGS)\n",
    "\n",
    "cities = {}\n",
    "try:\n",
    "    # Load the CSV file in memory using pandas\n",
    "    cities_df = pd.read_csv(CITY_PATH)\n",
    "    for _, row in cities_df.iterrows():\n",
    "        # Convert city_ascii to string and skip if it's NaN\n",
    "        if pd.isna(row['city_ascii']):\n",
    "            continue\n",
    "        \n",
    "        city_id = quote(row['city_ascii'].encode('ascii', 'ignore').decode().replace(\" \", \"_\").replace(\"'\", \"\").replace(\",\", \"\")+str(hash(row['lat']+row['lng'])))\n",
    "        cities[city_id] = {\n",
    "            'name': row['city_ascii'],\n",
    "            'population': str(row['population']) if pd.notna(row['population']) else \"0\",\n",
    "            'state_name': str(row['state_name']) if pd.notna(row['state_name']) else \"\",\n",
    "            'state_id': str(row['state_id']) if pd.notna(row['state_id']) else \"\",\n",
    "            'lat': str(row['lat']) if pd.notna(row['lat']) else \"\",\n",
    "            'lng': str(row['lng']) if pd.notna(row['lng']) else \"\"\n",
    "        }\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Add cities to the city graph\n",
    "for city_id, data in cities.items():\n",
    "    # Remove special characters and spaces, then URL encode\n",
    "    city_uri = FDO[city_id]\n",
    "    \n",
    "    # Add city triples\n",
    "    g_city.add((city_uri, RDF.type, FDO.City))\n",
    "    g_city.add((city_uri, FDO.name, Literal(data['name'], datatype=XSD.string)))\n",
    "    g_city.add((city_uri, FDO.population, Literal(data['population'], datatype=XSD.nonNegativeInteger)))\n",
    "    g_city.add((city_uri, FDO.latitude, Literal(data['lat'], datatype=WGS.lat)))\n",
    "    g_city.add((city_uri, FDO.longitude, Literal(data['lng'], datatype=WGS.long)))\n",
    "\n",
    "    if data['state_name'] and any(state['name'] == data['state_name'] for state in states):  # Check if state_name exists and is in states\n",
    "        g_city.add((city_uri, FDO.isLocatedInState, FDO[data['state_name'].encode('ascii', 'ignore').decode().replace(\" \", \"_\").replace(\"'\", \"\").replace(\",\", \"\")]))\n",
    "\n",
    "g_city.serialize(destination=str(OUTPUT_CITY_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_airport = Graph()\n",
    "\n",
    "g_airport.bind(\"fdo\", FDO)\n",
    "g_airport.bind(\"xsd\", XSD)\n",
    "g_airport.bind(\"wgs\", WGS)\n",
    "\n",
    "# Define the isLocatedInCity property in the airport graph\n",
    "g_airport.add((FDO.isLocatedInCity, RDF.type, RDF.Property))\n",
    "g_airport.add((FDO.isLocatedInCity, RDFS.domain, FDO.Airport))\n",
    "g_airport.add((FDO.isLocatedInCity, RDFS.range, FDO.City))\n",
    "\n",
    "airports = []\n",
    "try:\n",
    "    with open(AIRPORT_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['IATA']:  # Only include airports with IATA code\n",
    "                airports.append({\n",
    "                    'name': row['Name'].strip('\"'),\n",
    "                    'city': row['City'].strip('\"'),\n",
    "                    'iata': row['IATA'].strip('\"'),\n",
    "                    'lat': row['LAT'].strip('\"'),\n",
    "                    'lng': row['LONG'].strip('\"')\n",
    "                })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading airports file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "index = 0\n",
    "# Add airports and their relationships to the airport graph\n",
    "for airport in airports:\n",
    "    # Use IATA code directly in the URI\n",
    "    airport_uri = URIRef(str(FDO) + airport['iata'])\n",
    "    \n",
    "    # Add airport triples\n",
    "    g_airport.add((airport_uri, RDF.type, FDO.Airport))\n",
    "    g_airport.add((airport_uri, FDO.name, Literal(airport['name'], datatype=XSD.string)))\n",
    "    g_airport.add((airport_uri, FDO.latitude, Literal(airport['lat'], datatype=WGS.lat)))\n",
    "    g_airport.add((airport_uri, FDO.longitude, Literal(airport['lng'], datatype=WGS.long)))\n",
    "    \n",
    "    # Find the closest city to the airport\n",
    "    closest_city_uri = None\n",
    "    min_distance = float('inf')\n",
    "    citiesuris = [city_id for city_id, data in cities.items() if data['name'] == airport['city']]\n",
    "    \n",
    "    for city_id in citiesuris:\n",
    "        city_lat = float(cities[city_id]['lat'])\n",
    "        city_lng = float(cities[city_id]['lng'])\n",
    "        # Calculate the distance between the city and the airport\n",
    "        distance = GD((city_lat, city_lng), (float(airport['lat']), float(airport['lng']))).km\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_city_uri = FDO[city_id] \n",
    "    city_uri = closest_city_uri\n",
    "\n",
    "    if city_uri:\n",
    "        g_airport.add((airport_uri, FDO.isLocatedInCity, city_uri))\n",
    "     \n",
    "# Serialize both graphs to separate TTL files\n",
    "g_airport.serialize(destination=str(OUTPUT_AIRPORT_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_carrier = Graph()\n",
    "\n",
    "g_carrier.bind(\"fdo\", FDO)\n",
    "g_carrier.bind(\"xsd\", XSD)\n",
    "\n",
    "# populate the carrier graph\n",
    "carriers = []\n",
    "try:\n",
    "    with open(CARRIER_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['IATA']:  # Only include carriers with IATA code\n",
    "                carriers.append({\n",
    "                    'name': row['Name'].strip('\"'),\n",
    "                    'iata': row['IATA'].strip('\"'),\n",
    "                    'callsign': row['Callsign'].strip('\"'),\n",
    "                    'airline_id': row['\\ufeffAirlineID'].strip('\"')  # Updated to match exact column name with BOM\n",
    "                })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading carriers file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "for carrier in carriers:\n",
    "    carrier_uri = FDO[carrier['iata']]\n",
    "    g_carrier.add((carrier_uri, RDF.type, FDO.Carrier))\n",
    "    if carrier['callsign']:\n",
    "        g_carrier.add((carrier_uri, FDO.callSign, Literal(carrier['callsign'], datatype=XSD.string)))\n",
    "    g_carrier.add((carrier_uri, FDO.name, Literal(carrier['name'], datatype=XSD.string)))\n",
    "\n",
    "g_carrier.serialize(destination=str(OUTPUT_CARRIER_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = hashlib.sha1() \n",
    "\n",
    "g_manufacturer = Graph()\n",
    "\n",
    "g_manufacturer.bind(\"fdo\", FDO)\n",
    "g_manufacturer.bind(\"xsd\", XSD)\n",
    "\n",
    "manufacturers = []\n",
    "try:\n",
    "   with open(MANUFACTURER_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            hasher.update(row['ManufacturerName'].strip().encode('utf-8'))\n",
    "            manufacturer_id = 'mf'+hasher.hexdigest()[:16]\n",
    "            manufacturers.append({\n",
    "                'manufacturer_id': manufacturer_id,\n",
    "                'manufacture_code': row['ManufactureCode'].strip(),\n",
    "                'manufacturer_name': row['ManufacturerName'].strip()\n",
    "            })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading aircrafts file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "index = 0\n",
    "for manufacturer in manufacturers:\n",
    "    if (FDO[manufacturer['manufacturer_id']], RDF.type, FDO.Manufacturer) not in g_manufacturer:\n",
    "        manufacturer_uri = FDO[manufacturer['manufacturer_id']]\n",
    "        g_manufacturer.add((manufacturer_uri, RDF.type, FDO.Manufacturer))\n",
    "        g_manufacturer.add((manufacturer_uri, FDO.name, Literal(manufacturer['manufacturer_name'], datatype=XSD.string)))\n",
    "\n",
    "    print(f\"{int(index/len(manufacturers)*100)} % complete \\r\", end=\"\")\n",
    "    index+=1\n",
    "\n",
    "g_manufacturer.serialize(destination=str(OUTPUT_MANUFACTURER_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Graph()\n",
    "\n",
    "g_model.bind(\"fdo\", FDO)\n",
    "g_model.bind(\"xsd\", XSD)\n",
    "\n",
    "models = []\n",
    "try:\n",
    "    with open(MODEL_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            models.append({\n",
    "                'model_code': row['ModelCode'].strip(),\n",
    "                'model_name': row['ModelName'].strip(),\n",
    "            })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading model file: {str(e)}\")\n",
    "    raise\n",
    "index = 0\n",
    "for model_data in models:\n",
    "    model_uri = URIRef(str(FDO) + quote(model_data['model_code']))\n",
    "    g_model.add((model_uri, RDF.type, FDO.Model))\n",
    "    g_model.add((model_uri, FDO.name, Literal(model_data['model_name'], datatype=XSD.string)))\n",
    "    g_model.add((model_uri, FDO.modelCode, Literal(model_data['model_code'], datatype=XSD.string)))\n",
    "\n",
    "    manufacturer_uri = None\n",
    "    for manufacturer in manufacturers:\n",
    "        if manufacturer['manufacture_code'] == model_data['model_code']:\n",
    "            manufacturer_uri = FDO[manufacturer['manufacturer_id']]\n",
    "            g_model.add((model_uri, FDO.hasManufacturer, manufacturer_uri))\n",
    "            break\n",
    "\n",
    "    print(f\"{int(index/len(models)*100)} % complete \\r\", end=\"\")\n",
    "    index+=1\n",
    "\n",
    "print(f\"serialization \\r\")\n",
    "g_model.serialize(destination=str(OUTPUT_MODEL_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_aircraft = Graph()\n",
    "\n",
    "g_aircraft.bind(\"fdo\", FDO)\n",
    "g_aircraft.bind(\"xsd\", XSD)\n",
    "\n",
    "aircrafts = []\n",
    "try:\n",
    "    with open(AIRCRAFT_PATH, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            aircrafts.append({\n",
    "                'n_number': row['AircraftID'],\n",
    "                'model_code': row['ModelCode'].strip(),\n",
    "                'aircraft_type': row['AircraftType'].strip(),\n",
    "                'register_city': row['RegisterCity'].strip()\n",
    "            })\n",
    "except Exception as e:\n",
    "    print(f\"Error reading aircrafts file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "index = 0\n",
    "for aircraft in aircrafts:\n",
    "    encoded_n_number = quote(aircraft['n_number'].strip())\n",
    "    aircraft_uri = FDO[encoded_n_number]\n",
    "    g_aircraft.add((aircraft_uri, RDF.type, FDO.Aircraft))\n",
    "    g_aircraft.add((aircraft_uri, FDO.aircraftType, Literal(aircraft['aircraft_type'], datatype=XSD.string)))\n",
    "\n",
    "    for model in models:\n",
    "        if aircraft['model_code'] == model['model_code']:\n",
    "            model_code = aircraft['model_code']\n",
    "            model_uri = FDO[model_code]\n",
    "            g_aircraft.add((aircraft_uri, FDO.hasModel, model_uri))\n",
    "    print(f\"{int(index/len(aircrafts)*100)} % complete \\r\", end=\"\")\n",
    "    index+=1\n",
    "\n",
    "print(f\"serialization \\r\")\n",
    "g_aircraft.serialize(destination=str(OUTPUT_AIRCRAFT_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression of the meteostat warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "g_weather = Graph()\n",
    "\n",
    "g_weather.bind(\"fdo\", FDO)\n",
    "g_weather.bind(\"xsd\", XSD)\n",
    "\n",
    "# for each airport, get the weather data\n",
    "index = 0\n",
    "for airport in airports:\n",
    "    # Try to get the weather data for each airport\n",
    "    try:   \n",
    "        # Get the weather data\n",
    "        point = Point(float(airport['lat']), float(airport['long']), 0)\n",
    "        data = Hourly(point, start, end)\n",
    "        data = data.fetch()\n",
    "        \n",
    "        # Create the node to add to the Graph\n",
    "        for idx, row in data.iterrows():\n",
    "            # the node has the namespace + the airport iata + timestamp as URI\n",
    "            airport_uri = URIRef(str(FDO) + airport['iata'] + str(int(row.name.timestamp()/1000)))\n",
    "            \n",
    "            # Add airport triples\n",
    "            g_weather.add((airport_uri, RDF.type, FDO.Weather))\n",
    "            g_weather.add((airport_uri, FDO['weatherDate'], Literal(int(row.name.timestamp()), datatype=XSD.dateTime)))\n",
    "            coco_standardized = 0 if pd.isna(row.coco) else int(row.coco)\n",
    "            g_weather.add((airport_uri, FDO['weatherType'], Literal(coco_standardized, datatype=XSD.nonNegativeInteger)))\n",
    "            g_weather.add((airport_uri, FDO['hasAirport'], URIRef(FDO[airport['iata']])))\n",
    "    except Exception as e:\n",
    "        print(f\"Error weather ({airport['iata']}): {str(e)}\")\n",
    "\n",
    "    print(f\"{int(index/len(airports)*100)} % complete \\r\", end=\"\")\n",
    "    index+=1\n",
    "\n",
    "print(f\"serialization \\r\")\n",
    "g_weather.serialize(destination=str(OUTPUT_WEATHER_PATH), format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights and routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_flights = Graph()\n",
    "g_routes = Graph()\n",
    "\n",
    "g_flights.bind(\"fdo\", FDO)\n",
    "g_flights.bind(\"xsd\", XSD)\n",
    "g_routes.bind(\"fdo\", FDO)\n",
    "g_routes.bind(\"xsd\", XSD)\n",
    "\n",
    "timefieldsontology = {\n",
    "        'Scheduled departure time as shown in Official Airline Guide(OAG)': 'ScheduledDepartureOAGTime',\n",
    "        'Scheduled departure time as shown in CRS(selected by the Carrier)': 'ScheduledDepartureCRSTime',\n",
    "        'Gate departure time (actual)': 'ActualGateDepartureTime',\n",
    "        'Scheduled arrival time per OAG': 'ScheduledArrivalTimePerOAG',\n",
    "        'Scheduled arrival time per CRS': 'ScheduledArrivalTimePerCRS',\n",
    "        'Gate arrival time (actual)': 'ActualGateArrivalTime',\n",
    "        'Wheels-off time (actual)': 'ActualWheels-offTime',\n",
    "        'Wheels-on time (actual)': 'ActualWheels-onTime'\n",
    "    }\n",
    "minutesfieldsontology = {\n",
    "    'Scheduled elapsed time per CRS': 'ScheduledElapsedTimePerCRS',\n",
    "    'Actual gate-to-gate time': 'ActualGate-to-gateTime',\n",
    "    'Departure delay time (actual minutes)': 'ActualDepartureDelayTime',\n",
    "    'Arrival delay time (actual minutes)': 'ActualArrivalDelayTime',\n",
    "    'Elapsed time difference (actual minutes)': 'ActualElapsedTimeDifference',\n",
    "    'Minutes late for Delay Code E - Carrier Caused': 'LateE',\n",
    "    'Minutes late for Delay Code F - Weather': 'LateF',\n",
    "    'Minutes late for Delay Code G - National Aviation System (NAS)': 'LateG',\n",
    "    'Minutes late for Delay Code H - Security': 'LateH',\n",
    "    'Minutes late for Delay Code I - Late Arriving Flight (Initial)': 'LateI'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Load the CSV file in memory using pandas\n",
    "    flights_df = pd.read_csv(FLIGHT_PATH, dtype={\"Scheduled Operating Carrier Code\": \"string\", \"Date of flight operation\": \"string\"})\n",
    "    # Fill NaN values in \"Actual Operating Carrier Flight Number\" with 0 and convert to int\n",
    "    flights_df[\"Actual Operating Carrier Flight Number\"] = flights_df[\"Actual Operating Carrier Flight Number\"].fillna(0).astype(int)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "index = 0\n",
    "# Add flights to the flights graph\n",
    "for idx, row in flights_df.iterrows():\n",
    "\n",
    "    route_id = quote(str(row['Departure airport code']) + str(row['Arrival airport code']))\n",
    "    route_uri = URIRef(str(FDO) + route_id)\n",
    "\n",
    "    # Add route triples\n",
    "    # check if the route already has the carrier\n",
    "    if (route_uri, FDO['hasCarrier'], URIRef(FDO[row['Marketing Carrier code']])) not in g_routes:\n",
    "        # check if the route already exists\n",
    "        if (route_uri, RDF.type, FDO.Route) not in g_routes:\n",
    "            g_routes.add((route_uri, RDF.type, FDO.Route))\n",
    "            g_routes.add((route_uri, FDO['hasDepartureAirport'], URIRef(FDO[row['Departure airport code']])))\n",
    "            g_routes.add((route_uri, FDO['hasArrivalAirport'], URIRef(FDO[row['Arrival airport code']])))\n",
    "        # add the carrier to the route\n",
    "        g_routes.add((route_uri, FDO['hasCarrier'], URIRef(FDO[row['Marketing Carrier code']])))\n",
    "\n",
    "    flight_date = datetime.strptime(row['Date of flight operation'], \"%m/%d/%Y\")\n",
    "    flight_id = quote(str(row['Actual Operating Carrier Code']) + str(row['Actual Operating Carrier Flight Number']) + flight_date.strftime(\"%Y%m%d\"))\n",
    "    flight_uri = URIRef(str(FDO) + flight_id)\n",
    "\n",
    "    start_time = row['Scheduled departure time as shown in Official Airline Guide(OAG)']\n",
    "    for time in timefieldsontology.keys():\n",
    "        if pd.isna(row[time]) or row[time] == 0:\n",
    "            row[time] = None\n",
    "            continue\n",
    "        # convert the time to a datetime object\n",
    "        m = int(str(int(row[time]))[-2:])\n",
    "        h = int(str(int(row[time]))[:2]) if len(str(row[time])) == 4 else 0\n",
    "        # check if the time is in the next day\n",
    "        d = 1 if row[time] < start_time-200 else 0\n",
    "        row[time] = flight_date + timedelta(days=d, hours=h, minutes=m)\n",
    "\n",
    "    # Add flights triples\n",
    "    g_flights.add((flight_uri, RDF.type, FDO.Flight))\n",
    "    g_flights.add((flight_uri, FDO['hasAircraft'], URIRef(FDO[str(row['Aircraft tail number'])])))\n",
    "    g_flights.add((flight_uri, FDO['hasRoute'], route_uri))\n",
    "    g_flights.add((flight_uri, FDO['isOperatedBy'], URIRef(FDO[str(row['Actual Operating Carrier Code'])])))\n",
    "    g_flights.add((flight_uri, FDO['isSoldBy'], URIRef(FDO[str(row['Marketing Carrier Code'])])))\n",
    "    if not pd.isna(row['Cancellation code']):\n",
    "        g_flights.add((flight_uri, FDO['CancellationCode'], Literal(row['Cancellation code'], datatype=XSD.string)))\n",
    "    g_flights.add((flight_uri, FDO['flightDate'], Literal(flight_date, datatype=XSD.dateTime)))\n",
    "\n",
    "    # time fields\n",
    "    for original, ontology in timefieldsontology.items():\n",
    "        if not pd.isna(row[original]):\n",
    "            g_flights.add((flight_uri, FDO[ontology], Literal(row[original], datatype=XSD.dateTime)))\n",
    "    # minutes fields\n",
    "    for original, ontology in minutesfieldsontology.items():\n",
    "        if not pd.isna(row[original]):\n",
    "            g_flights.add((flight_uri, FDO[ontology], Literal(int(row[original]), datatype=XSD.integer)))\n",
    "\n",
    "    index+=1\n",
    "    print(f\"{int(index/len(flights_df)*100)} % complete \\r\", end=\"\\r\")\n",
    "\n",
    "print(f\"serialization\\r\")\n",
    "g_flights.serialize(destination=str(OUTPUT_FLIGHT_PATH), format='turtle')\n",
    "g_routes.serialize(destination=str(OUTPUT_ROUTE_PATH), format='turtle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
